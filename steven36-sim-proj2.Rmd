---
title: 'Week 6 - Simulation Project'
author: "STAT 420, Summer 2020, D. Unger"
date: ''
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
urlcolor: cyan
---

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80)
library(knitr)
opts_chunk$set(cache = TRUE, autodep = TRUE)
```



```{r}
birthday = 030370
set.seed(birthday)
```

# Simulation Study 1: Significance of Regression

In this simulation study we will investigate the significance of regression test. We will simulate from two different models:

1. The **"significant"** model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 3$,
- $\beta_1 = 1$,
- $\beta_2 = 1$,
- $\beta_3 = 1$.


2. The **"non-significant"** model

\[
Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 3$,
- $\beta_1 = 0$,
- $\beta_2 = 0$,
- $\beta_3 = 0$.

For both, we will consider a sample size of $25$ and three possible levels of noise. That is, three values of $\sigma$.

- $n = 25$
- $\sigma \in (1, 5, 10)$

Use simulation to obtain an empirical distribution for each of the following values, for each of the three values of $\sigma$, for both models.

- The **$F$ statistic** for the significance of regression test.
- The **p-value** for the significance of regression test
- **$R^2$**

For each model and $\sigma$ combination, use $2000$ simulations. For each simulation, fit a regression model of the same form used to perform the simulation.

Use the data found in [`study_1.csv`](study_1.csv) for the values of the predictors. These should be kept constant for the entirety of this study. The `y` values in this data are a blank placeholder.

Done correctly, you will have simulated the `y` vector $2 (models)×3 (sigmas)×2000 (sims)=12000$ times.

Potential discussions:

- Do we know the true distribution of any of these values?
- How do the empirical distributions from the simulations compare to the true distributions? (You could consider adding a curve for the true distributions if you know them.)
- How are each of the $F$ statistic, the p-value, and $R^2$ related to $\sigma$? Are any of those relationships the same for the significant and non-significant models?




```{r}

n = 25
p = 4 #no. of parameters
dfx = read.csv("study_1.csv")
X_pred = as.matrix(cbind("x0" = rep(1, n), dfx[ ,2:ncol(dfx)]))
S = c(1, 5, 10)  

#function input: n = #obs, B = beta vector, X = predictor matrix w leading 1s, sigma = stdev, df = dataframe of x(pred) and y values
#function output: a vector of F stat, p-value, Rsquared 
get_vals = function(n, B, X, sigma, df) {
  noise = rnorm(n, mean = 0, sd = sigma)   
  y = X %*% B + noise
  df[ , 1] = as.vector(y)
  m = lm(y ~ ., data = df)
  f = summary(m)$fstatistic
  p = pf(q = f[1], df1 = f[2], df2 = f[3], lower.tail = FALSE)
  rsq = summary(m)$r.squared
  vec = c(f[1], p, rsq)
  as.vector(vec)
}
#matrices to store data of sig and non-sig model
vals0 = matrix(0, 2000, 9)  #non-sig model
vals1 = matrix(0, 2000, 9)  #sig model

#loops to simulate model 2000 times for 3 different noise levels
k=0
for (i in 1:3) {
  a = i+k
  b = i+k+2
  for (j in 1:2000) {
    vals1[ j, a : b] = get_vals(n, B = c(3,1,1,1), X_pred, S[i], dfx)
    vals0[ j, a : b] = get_vals(n, B = c(3,0,0,0), X_pred, S[i], dfx)
  }
  k=k+2
}
```

```{r}
#pval
par(mfrow = c(1,3))
hist(vals1[ , 2], breaks = 30)
hist(vals1[ , 5], breaks = 30)
hist(vals1[ , 8], breaks = 30)

c(mean(vals1[ ,2]), mean(vals1[ ,5]), mean(vals1[ ,8]))
  
```


```{r}
#Fstat
par(mfrow = c(1,3))
hist(vals1[ , 1], breaks = 30)
hist(vals1[ , 4], breaks = 30)
hist(vals1[ , 7], breaks = 30)

c(mean(vals1[ ,1]), mean(vals1[ ,4]), mean(vals1[ ,7]))


```

```{r}
#Rsq
par(mfrow = c(1,3))
hist(vals1[ , 3], breaks = 30)
hist(vals1[ , 6], breaks = 30)
hist(vals1[ , 9], breaks = 30)

c(mean(vals1[ ,3]), mean(vals1[ ,6]), mean(vals1[ ,9]))


```

```{r}
#Fstat

par(mfrow = c(1,3))
hist(vals0[ , 1], breaks = 30)
hist(vals0[ , 4], breaks = 30)
hist(vals0[ , 7], breaks = 30)

c(mean(vals0[ ,1]), mean(vals0[ ,4]), mean(vals0[ ,7]))
```


```{r}
#pval

par(mfrow = c(1,3))
hist(vals0[ , 2], breaks = 30)
hist(vals0[ , 5], breaks = 30)
hist(vals0[ , 8], breaks = 30)

c(mean(vals0[ ,2]), mean(vals0[ ,5]), mean(vals0[ ,8]))
```

```{r}
#Rsq

par(mfrow = c(1,3))
hist(vals0[ , 3], breaks = 30)
hist(vals0[ , 6], breaks = 30)
hist(vals0[ , 9], breaks = 30)

c(mean(vals0[ ,3]), mean(vals0[ ,6]), mean(vals0[ ,9]))
```



### Introduction

One of the goals of this study is to examine the role of noise as a function of the F statistic and p-value in the context of determining the significance of the regression model.  The regression model has 3 predictors, and this study examines two different sets of beta parameters. The null hypothesis is, $H_0:  \beta_1 = \beta_2 = \beta_3 = 0$ and the alternate hypothesis is, $H_1: at\ least\ one\ \beta_1, \beta_2, \beta_3 \ne 0$.  Furthmore, $R^2$, will also be calculated to determine its effect as a function of noise.      

### Methods

To carry out this study, we defined a true linear model where the beta parameters and sigma are known.  The true model was defined as: 

  $Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \epsilon_i$ 
  
  where $\epsilon_i \sim N(0, \sigma^2)$ and the two sets of beta parameters are,
  
Significant Model:
- $\beta_0 = 3$,
- $\beta_1 = 1$,
- $\beta_2 = 1$,
- $\beta_3 = 1$, 

Non-Significant Model:
- $\beta_0 = 3$,
- $\beta_1 = 0$,
- $\beta_2 = 0$,
- $\beta_3 = 0$.

To obtain empirical data we performed simulations using the true model and added noise to it.  The amount of noise was varied by $\sigma \in (1, 5, 10)$.  The x values, predictors, were fixed and obtained from study_1.csv file.  A function called get_Anova() was created to obtain the F statistic, p-value, and $R^2$ value.  It inputs the beta vector, X input matrix, and noise level as parameters and creates both the null and full model using the lm() function.  The anova() function was used to calculate the F statistic and p-value from these two models.  This was performed 2000 times for each noise level.  

### Results

### Discussion


In the significant model (beta parameters are non-zero), as noise increased the significance of regression decreased.  This is supported by the decrease in the F statistic and increase in the p-value as noise increases, as shown in the histograms.  When noise level is at its lowest, sigma = 1, the p-value is extremely low. The mean is < 1e-6.  This tells us that we should reject the null hypothesis and that at least one of the predictors has a linear relationship with y when all three predictors, x1, x2, and x3 are included in the analysis.  We should expect this because the true model has non-zero beta parameters.  At noise levels $\sigma = \sqrt{5}\ and\ \sqrt{10}$ the mean p-values are 0.008 and 0.06, respectively.  Assuming we set the probability criteria to $\alpha = 0.05$, a greater portion of the simulation values are starting to fail to reject the null hypothesis due to increased noise level.  

We should expect this since noise increases the spread of y.  This can affect the F statistic by reducing the SSReg.  The increased noise will cause a worse regression fit thereby reducing the SSReg value.  The SSE can also be affected by a larger spread causing the error to increase.  The combined effects will reduce the F statistic which is given by F = SSReg/SSE.  A smaller F statistic will in turn result in a high p-value given by the F distribution with df1 = p-1 and df2 = n-p.    

SSE (sum of squares error).  As SSE increases the F statistic decreases which can be shown by F = SSReg/SSE. The Rsquared values collected in this study also supports the summary above.  As noise increases the Rsquared value decreases which translates to less of the variation in y can be explained by the regression model.  This can be seen in the histogram plots.  Mathematically speaking, as noise increases SSE also increases which decreases Rsquared since Rsquared = 1 - SSE/SST.     

If we look at the non-significant model where all beta parameters are zero (excluding beta intercept),  we notice that most of the time we fail to reject the null hypothesis, as should be the case since the non-intercept beta values are all zero in the true model.  On average, the F statistic was 1.1 and the p-value was 0.5.  The Rsquared value was quite low at 0.12. Another thing to note, in contrast to the significant model, the mean values of the F statistic, p-value and Rsquared values were little affected by the noise level.  The explaination is that there was no significance in the regression model to start with...The response is essentially predicted by the mean of y regardless the of x values.  


# Simulation Study 2: Using RMSE for Selection?

### Introduction
In this study we examine RMSE as a metric to evaluate a multiple linear regression model using both training and test data sets.  Simulation was used to conduct the study where a true model was defined and data was generated from it along with a noise component.        

### Methods
The true model is defined by, $Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + \beta_5 x_{i5} + \beta_6 x_{i6} + \epsilon_i$  

where $\epsilon_i \sim N(0, \sigma^2)$ and

- $\beta_0 = 0$,
- $\beta_1 = 3$,
- $\beta_2 = -4$,
- $\beta_3 = 1.6$,
- $\beta_4 = -1.1$,
- $\beta_5 = 0.7$,
- $\beta_6 = 0.5$.

A sample size of $500$ was used and three possible levels of noise were tested, that is, three values of $\sigma$.

- $n = 500$
- $\sigma \in (1, 2, 4)$

The file "study_2.csv" was used for the values of the predictors and were kept constant for the entirety of this study.
Each time data was simulated, the data set was randomly split into train and test sets of equal sizes (250 observations for training, 250 observations for testing).

Nine models, with forms:

- `y ~ x1`
- `y ~ x1 + x2`
- `y ~ x1 + x2 + x3`
- `y ~ x1 + x2 + x3 + x4`
- `y ~ x1 + x2 + x3 + x4 + x5`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6`, the correct form of the model as noted above
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8`
- `y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9`

were fit. For each model, the train and test RMSE were calculated.

\[
\text{RMSE}(\text{model, data}) = \sqrt{\frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2}
\]

A thousand simulations for each of the $3$ values of $\sigma$ were performed.

The code to perform the tasks above are shown below.

```{r}
study2 = read.csv("study_2.csv")
B = c(0, 3, -4, 1.6, -1.1, 0.7, 0.5)
S = c(1, 2, 4)  

#this function shuffles and splits the study2 dframe, trn and tst dframe stored in a list
#input:df=dataframe to split, ratio=split ratio relative to trn data
#output: train and test data sets wrapped in a list
train_test = function(df, ratio) {
  n = nrow(df)
  ncols = ncol(df)
  n_train = n * ratio
  shuffle_index = sample(1:n, n)
  train = data.frame(matrix(0, n_train, ncols))
  colnames(train) = c("y", "x1", "x2", "x3", "x4", "x5", "x6", "x7", "x8", "x9")
  test = data.frame(matrix(0, n - n_train, ncols))
  colnames(test) = c("y", "x1", "x2", "x3", "x4", "x5", "x6", "x7", "x8", "x9")
  for (i in 1:n_train) {
    indx = shuffle_index[i]
    train[i,] = df[indx, ]
  }
  for (i in 1 : n - n_train) {
    indx = shuffle_index[n_train + i]
    test[i, ] = df[indx, ]
  }
  list(train, test)
}

```

```{r}

#function to compute rmse 
#function input: B = beta vector, sigma = stdev, df = dataframe 
#function output: test and rmse wrapped in a list 
get_rmse = function(B, sigma, df, ratio) {
  n = nrow(df)
  X = as.matrix(cbind("x0" = rep(1, n), df[ ,2:7])) #this is for true model, only betas x0-x6 required
  noise = rnorm(n, mean = 0, sd = sigma)   
  y = X %*% B + noise
  df[ , 1] = as.vector(y)
  temp = train_test(df, ratio)  
  trn = temp[[1]] #this is a data frame
  tst = temp[[2]]
  n_trn = nrow(trn)  #n_train in this case is 250 
  n_tst = nrow(tst)
  m1 = lm(y ~ x1, data = trn)
  m2 = lm(y ~ x1 + x2, data = trn)
  m3 = lm(y ~ x1 + x2 + x3, data = trn)  
  m4 = lm(y ~ x1 + x2 + x3 + x4, data = trn)  
  m5 = lm(y ~ x1 + x2 + x3 + x4 + x5, data = trn)  
  m6 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6, data = trn)  
  m7 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7, data = trn)  
  m8 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = trn)  
  m9 = lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9, data = trn)  
  models = list(m1, m2, m3, m4, m5, m6, m7, m8, m9)
  rmse_train = rep(0, length(models))
  rmse_test = rep(0, length(models))
  for (i in 1:length(models))  {
    err = resid(models[[i]])
    rmse_trn = sqrt(sum(err^2)/n_trn) 
    yhat = predict(models[[i]], newdata = tst)
    e = yhat - tst[ , 1]
    rmse_tst = sqrt(sum(e^2)/n_tst)
    rmse_train[i] = rmse_trn
    rmse_test[i] = rmse_tst
  } 
  list(rmse_train, rmse_test)
}

```


```{r}
performSim = function(B, sigma, df, ratio, n_sim, n_model) {
  n_column = n_model * 2
  n_modelp1 = n_model + 1
  data = matrix(0, n_sim, n_column)
  for (i in 1:n_sim) {
    temp = get_rmse(B, sigma, df, ratio)
    data[i, 1:n_model] = temp[[1]]
    data[i, n_modelp1:n_column] = temp[[2]]
  }
  data
}
```

```{r, eval=TRUE}
#perform simulations
data_sig1 = performSim(B, sigma = S[1], df = study2, ratio = 0.5, n_sim = 1000, n_model = 9)
data_sig2 = performSim(B, sigma = S[2], df = study2, ratio = 0.5, n_sim = 1000, n_model = 9)
data_sig3 = performSim(B, sigma = S[3], df = study2, ratio = 0.5, n_sim = 1000, n_model = 9)
```

### Results

```{r}
par(mfrow = c(1,2))
boxplot(data_sig1[, 1:9], xlab = "Model No.", ylab = "RMSE", main = "Train RMSE, sigma = 1", col = "cyan")
boxplot(data_sig1[ , 10:18],xlab = "Model No.", ylab = "RMSE", main = "Test RMSE, sigma = 1", col = "orange")
```

```{r}
par(mfrow = c(1,2))
boxplot(data_sig2[, 1:9], xlab = "Model No.", ylab = "RMSE", main = "Train RMSE, sigma = 2", col = "cyan")
boxplot(data_sig2[ , 10:18],xlab = "Model No.", ylab = "RMSE", main = "Test RMSE, sigma = 2", col = "orange")
```

```{r}
par(mfrow = c(1,2))
boxplot(data_sig3[, 1:9], xlab = "Model No.", ylab = "RMSE", main = "Train RMSE, sigma = 4", col = "cyan")
boxplot(data_sig3[ , 10:18],xlab = "Model No.", ylab = "RMSE", main = "Test RMSE, sigma = 4", col = "orange")
```

```{r}
#on average, which model has lowest Test RMSE 
a = apply(data_sig1[,10:18], 2, mean)
b = apply(data_sig2[,10:18], 2, mean)
c = apply(data_sig3[,10:18], 2, mean)
#data.frame(sigma1 = which.min(a), sigma2 = which.min(b), sigma4 =which.min(c))
d = rbind("Test RMSE - sigma1" = a, "Test RMSE - sigma2" = b, "Test RMSE - sigma4" = c)
d = data.frame(d)
colnames(d) = 1:9
d
```

```{r}
#on average, which model has lowest Train RMSE 
a = apply(data_sig1[,1:9], 2, mean)
b = apply(data_sig2[,1:9], 2, mean)
c = apply(data_sig3[,1:9], 2, mean)
#data.frame(sigma1 = which.min(a), sigma2 = which.min(b), sigma4 =which.min(c))
d = rbind("Train RMSE - sigma1" = a, "Train RMSE - sigma2" = b, "Train RMSE - sigma4" = c)
d = data.frame(d)
colnames(d) = 1:9
d
```

### Discussion
On average, the test RMSE was able to select the best model, model 6.  There are cases when model 6 does not have the lowest RMSE due to the random noise during simulation.  This can be seen on the box plots where other models can have a lower RMSE than model 6, but on average it will have the lowest RMSE.  The boxplot does not plot the mean but rather the median.  In our case, the median and mean are very close to each other.  

In contrast, when viewing the Train RMSE, the model with the lowest RMSE on average is model 9.  This is the model when all predictors are included or the model with the most flexibility.  One should not rely on the Train RMSE because the model is being fit to this data.  When the model has higher flexibility it can fit to the noise and will not perform well as a general model.  It is better to use the Test RMSE over Train RMSE because the model has never seen the test data and cannot be influenced by it.

Another important factor which affects the RMSE is the noise level.  By looking at the three sets of box plots, one for each noise level, one will notice the RMSE increases with noise.  Noise will inherently cause the error $y_i - \hat y_i$ to increase due to the larger spread in data.  


# Simulation Study 3: Power

### Introduction
In this study we will examine how power is influenced by noise, beta coefficient, and sample size.  Power is the probability of rejecting the null hypothesis given that the alternate hypothesis is true and $\beta_1$ is non-zero.  A simulation was carried out to perform the study rather than a mathematical derivation.  

### Methods
We defined a true linear regression equation with a known $\beta_1$ coefficients.  The true equation takes the form of, $y = \beta_0 + \beta_1x_i + \epsilon$ so by definition we know that the null hypothesis should be rejected.  However the noise term, $\epsilon$, will affect the behavior during the simulation.  By iterating over $\beta$ coefficients, noise level, and sample size we can evaluate how power is effected by these parameters.  

(Note: text below taken from assignment)
Essentially, power is the probability that a signal of a particular strength will be detected. Many things affect the power of a test. In this case, some of those are:

- Sample Size, $n$
- Signal Strength, $\beta_1$
- Noise Level, $\sigma$
- Significance Level, $\alpha$

We'll investigate the first three.

To do so we will simulate from the model

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

where $\epsilon_i \sim N(0, \sigma^2)$.

For simplicity, we will let $\beta_0 = 0$, thus $\beta_1$ is essentially controlling the amount of "signal." We will then consider different signals, noises, and sample sizes:

- $\beta_1 \in (-2, -1.9, -1.8, \ldots, -0.1, 0, 0.1, 0.2, 0.3, \ldots 1.9, 2)$
- $\sigma \in (1, 2, 4)$
- $n \in (10, 20, 30)$

We will hold the significance level constant at $\alpha = 0.05$.

The following code was used to generate the predictor values, `x`: values for different sample sizes.

```{r eval=FALSE}
x_values = seq(0, 5, length = n)
```

For each possible $\beta_1$ and $\sigma$ combination,  we simulated from the true model at least $1000$ times. Each time, the significance of the regression test was performed. To estimate the power with these simulations, the following expression was used, 

\[
\hat{\text{Power}} = \hat{P}[\text{Reject } H_0 \mid H_1 \text{ True}] = \frac{\text{# Tests Rejected}}{\text{# Simulations}}
\]

The code below performs the operations described above.

```{r}

beta = seq(-2, 2, .1)
S = c(1, 2, 4)  #sigma values

#function input: n = #obs, B = beta vector, sigma = stdev
#function output: single p-value
#notes: the X matrix (input data) is generated inside this function
get_pval = function(n, B, sigma) {
  x_values = seq(0, 5, length = n)
  df = data.frame("y" = rep(0,n), "x1" = x_values)
  X = as.matrix(cbind("x0" = rep(1, n), "x1" = x_values ))
  noise = rnorm(n, mean = 0, sd = sigma)   
  y = X %*% B + noise
  df[ , 1] = as.vector(y)
  m = lm(y ~ x1, data = df)
  pval = pf(q=summary(m)$fstatistic[1], df1 = length(coef(m)) - 1, df2 = n - length(coef(m)), lower.tail = FALSE)
}

#function input: n = #obs, B = beta vector, sigma = stdev, n_sim = no of simulations
#function output: vector of power values
#notes: perform simulations iterating over beta values 
simulate = function(n, beta, sigma, n_sim) {
  power_vec = c(0, length(beta))
  for (k in 1:length(beta)){
    B = c(0, beta[k])
    p_vec = rep(0, n_sim)
  
    for (i in 1:n_sim) {
      p_vec[i] = get_pval(n, B, sigma)
    }
    
    power_vec[k] = sum(p_vec < 0.05)/n_sim   #power = total Ho rejected/total simulations; criteria 0.05
  }
  power_vec
}
```

```{r}
#run simulations
p1 = simulate(n=10, beta, S[1], n_sim = 1000)
p2 = simulate(n=20, beta, S[1], n_sim = 1000)
p3 = simulate(n=30, beta, S[1], n_sim = 1000)

p4 = simulate(n=10, beta, S[2], n_sim = 1000)
p5 = simulate(n=20, beta, S[2], n_sim = 1000)
p6 = simulate(n=30, beta, S[2], n_sim = 1000)

p7 = simulate(n=10, beta, S[3], n_sim = 1000)
p8 = simulate(n=20, beta, S[3], n_sim = 1000)
p9 = simulate(n=30, beta, S[3], n_sim = 1000)

```

### Results

```{r}
plot(p1 ~ beta, xlab = "Beta_1 value", ylab = "Power",main = "Power vs. Beta Value - sigma = 1, n_sim = 1000", 
     col = "blue", type = "l", lwd = 2, lty = 1 )
lines(p2 ~ beta, col = "orange", type = "l", lwd = 2, lty = 2)
lines(p3 ~ beta, col = "magenta", type = "l", lwd = 2, lty = 3)
legend("bottomleft", legend = c("n = 10", "n = 20", "n = 30"), col = c("blue", "orange", "magenta"), lty = 1:3, cex = 1.1)
```

```{r}
plot(p4 ~ beta, xlab = "Beta_1 value", ylab = "Power",main = "Power vs. Beta Value - sigma = 2, n_sim = 1000", 
     col = "blue", type = "l", lwd = 2, lty = 1 )
lines(p5 ~ beta, col = "orange", type = "l", lwd = 2, lty = 2)
lines(p6 ~ beta, col = "magenta", type = "l", lwd = 2, lty = 3)
legend("bottomleft", legend = c("n = 10", "n = 20", "n = 30"), col = c("blue", "orange", "magenta"), lty = 1:3, cex = 1.1)
```

```{r}
plot(p7 ~ beta, xlab = "Beta_1 value", ylab = "Power",main = "Power vs. Beta Value - sigma = 4, n_sim = 1000", 
     col = "blue", type = "l", lwd = 2, lty = 1 )
lines(p8 ~ beta, col = "orange", type = "l", lwd = 2, lty = 2)
lines(p9 ~ beta, col = "magenta", type = "l", lwd = 2, lty = 3)
legend("bottomleft", legend = c("n = 10", "n = 20", "n = 30"), col = c("blue", "orange", "magenta"), lty = 1:3, cex = 1.1)
```


```{r}
p7_inc = simulate(n=10, beta, S[3], n_sim = 2000)
p8_inc = simulate(n=20, beta, S[3], n_sim = 2000)
p9_inc = simulate(n=30, beta, S[3], n_sim = 2000)
```


```{r}
plot(p7_inc ~ beta, xlab = "Beta_1 value", ylab = "Power",main = "Power vs. Beta Value - sigma = 4, n_sim = 2000", 
     col = "blue", type = "l", lwd = 2, lty = 1 )
lines(p8_inc ~ beta, col = "orange", type = "l", lwd = 2, lty = 2)
lines(p9_inc ~ beta, col = "magenta", type = "l", lwd = 2, lty = 3)
legend("bottomleft", legend = c("n = 10", "n = 20", "n = 30"), col = c("blue", "orange", "magenta"), lty = 1:3, cex = 1.1)
```

### Discussion
From all three plots we can see that power is affected by $\beta_1$, sample size and sigma.  As the $\beta_1$ values get closer to zero the power (signal) decreases.  In this case, x has little influence on the response which can be better predicted as simply $\bar y$.  The SSReg becomes very small since there is really no regression fit to the response.  In contrast, when $\B_1$ becomes larger the SSReg becomes larger as values on the regression line become further from $\bar y$.  This increases the F statistic which in turn results in a smaller p-value favoring to reject the null hypothesis and increasing power.     

Secondly, as sample size increases so does power.  The degree of freedom is increased with larger n, where df = n-p.  When the degrees of freedom increases in a F distribution a lower p-value will result for a given test statistic.  This in turn will improve our power results with higher n.

Lastly, when noise increases the power decreases, making it worse.  The shape of the plots become wider with lower power values with increased noise.  The noise increases the spread of the response, y.  This has the effect of increasing error which decreases the F statistic which in turn increases the p-value which makes the power worse. 

The power plot with sigma = 4 did not seem to converge as well as the others.  The plots were a little bit more jagged, especially for n=10.  By increasing the number of simulations, the power values seemed to converge better as indicated by a smoother plot.
















